\documentclass[11pt]{article} % For LaTeX2e
\usepackage{nips10submit_09,times}
\usepackage{cite}

\usepackage{ upgreek }
\usepackage{graphicx}
\usepackage{url}
\usepackage{verbatim} 

\newenvironment{itemize*}%
  {\begin{itemize}%
    \setlength{\itemsep}{0pt}%
    \setlength{\parskip}{0pt}}%
  {\end{itemize}}


\title{CS 6758: Semantic Labelling of Indoor Office Spaces using RGB-D Images}

\author{
Hema Swetha Koppula \\
 \texttt{hsk49@cornell.edu}
 \And
Abhishek Anand\\
 \texttt{aa755@cornell.edu}
}
%\author{
% }
\nipsfinalcopy
\begin{document}
\maketitle

\begin{abstract}
We aim to use the high quality RGB-D images obtained by kinect cameras to improve state of the art of semantic labelling of indoor office spaces.
To get started, we  collected kinect-videos of 11 office scenes, stitched them to form large point-cloud models and labelled them manually.
We trained and tested a very simple linear model which achieves 65 \% accuracy in labelling segments into 6 categories. In addition to this, we formulated 2 MRF models which use context to infer labels and worked out the learning and inference mechanisms. 
\end{abstract}

\section{Introduction}


  
  The availability of  RGB-D (Kinect-style) cameras \cite{kinect} which are capable of providing high quality synchronized 
  videos of both color and depth presents a great opportunity to combine color- and depth-based recognition. We aim 
  to use this rich new data for automated indoor scene interpretation using semantic context modelling. Given an RGB-D 
  image, we would like to label each point in the scene with the semantic label of the object to which the point belongs
  (eg. wall, floor, table, computer, etc). Many state-of-the-art object detectors make use of only features local to the region being 
  considered, but these approaches do not work for cluttered scenes. We plan to model the relations between various objects 
  in the scene and make use of this contextual information for scene interpretation. In addition to this, we are doing learning and
  inference on large stitched 3D models containing multiple views so that we have more context cues.

   \subsection{Goal}
     The goal of the project is scene understanding of indoor office spaces. Given an RGB-D image of a office scene, each point in the 
     image is classified to a semantic label. The various semantic labels to be detected are : wall, floor, ceiling, doors, tables, furniture 
     (chairs, sofas), office supplies (books, pens, markers), wearable\_on\_ground(shoes, etc.), wearable\_rest (jackets, caps,etc.), 
     containers (cups, mugs, water bottles), computer (monitor, keyboard, mouse, cpu, laptop), clutter (the rest). Once the semantic labelling 
     is performed in a given scene, this information can be used by the robot to perform various tasks like, finding objects and fetching them, 
     or maintaining an inventory of objects in the office building, etc.

\begin{comment}
   \subsection{Method}
     The image is first segmented into smaller point clouds based on segmentation using both image and point cloud features. We define a
     MRF structure, where each segment forms a node in the graph and each segment is connected to its k-nearest neighbours via relation links.
     The various relations we define between objects are: orthogonal, parallel, adjacent, coplanar, self/same-object, above, below, on-top-of, 
     in-front-of, beside, in-proximity, etc. The node potentials will depend on local visual and shape features, and the edge potentials will depend 
     on the contextual relations between the two nodes.

      
   \subsection{Evaluation}
     We evaluate our scene understanding method by measuring the accuracy of semantic labeling on the test set ie., percent of objects labelled 
     correctly. The test sets will comprise of different types of office spaces: 1. new office spaces which weren't used in the training of the model, 
     2. similar office spaces but new rooms, and 
     3. same office spaces but data collected on a different day. 
     
 \end{comment}
     
     
 \section{Related Work}
 
 
    It has been shown that contextual information significantly improve the performance of vision-based 
   object detectors \cite{Torralba:exploting_context, Hoiem:puttingobjects,Torralba:contextualmodels}. However 
   little work has focused on 3-d context. Hoiem et al. \cite{Hoiem:puttingobjects} and Leibe et al. \cite{Leibe07:dynamic} 
  infer the camera location and scene geometry from a single 2-d image or stereo video stream, respectively. 
  These works reconstruct rough geometry of street scenes (pedestrians and cars) and cannot, for example, 
  be used for estimating 3-d features of small objects.
  
    Some novel works use 3-d point clouds from depth sensors for detecting objects in both indoor and outdoor environments 
    \cite {xiong:indoor,Rusu:ObjectMaps , xiong:3DSceneAnalysis, Golovinskiy:shape-basedrecognition, Shapovalov2010}. 
   ~\cite{xiong:indoor} presents a CRF based method to discover and exploit contextual information, classifying planar patches 
   extracted from the point cloud data. They model the key structural components of building interiors (walls, floors, and ceilings) 
   and on distinguishing these components from other objects in the environment. The algorithm uses local features as well as 
   contextual relationships such as orthogonal, parallel, adjacent, and coplanar to label the patches.
  
   Gould et al.  \cite{gould:fusion} and Quigley et al. ~\cite{quigley:high-accuracy} demonstrate that augmenting state-of-the-art
   computer vision techniques with high-resolution 3D information can enhance object detection in cluttered real-world environments. 
   They merge the depth and visual data to obtain color/intensity, depth (location in 3-d space),  and surface normal information
   for each pixel. From this data, the method in \cite{gould:fusion} uses a sliding window approach to extract  2d image features, 
   contextual cues (such as height above the ground) and 3-d features (such as object size). They then train a binary logistic 
   classifier based on the features as object detectors. ~\cite{quigley:high-accuracy} perform object detection using a extension of 
   the sliding-window  approach to combine the visual and depth channels. They combine this with a door opening system and 
   perform an inventory-control experiment. 

   The recent availability of high quality synchronized videos of both color and depth obtained from RGB-D (Kinect-style) 
    depth cameras \cite{kinect}, shifted the focus to designing object detection techniques making use of both visual as well as 
    shape features ~\cite{lai:icra11a, lai:icra11b}. Henry et al. ~\cite{deter:rgbd} present a full 3D mapping system
    that utilizes a novel joint optimization algorithm combining visual features and shape-based alignment from RGB-D data.

\section{Our approach}
The first step is to stitch multiple views of an office to obtain a single colored pointcloud. 
For this, we use the RGBDSLAM package of ROS and post-process it's output remove duplicate points by using visibility constraints. Then we segment the pointcloud  into smaller point clouds based on normals(using a region growing approach). For each segment, we compute a battery of features. We call these as node features.
\begin{itemize}
\item histogram of eigen values of scatter matrices
\item histogram of color values
\item average spin-image
\item histogram of vertical component of normal
\end{itemize}

For pairs of nearby segments, we compute the following edge features
\begin{itemize}
\item distance between centroids
\item displacement between centroids
\item dot product between normals
\end{itemize}

 We define a MRF structure, where each segment forms a node in the graph and each segment is connected to its k-nearest neighbors via relation links. We formulated this model in 2 ways:
 
\subsection{Model Formulation 1}
For standard pair-wise Markov Network, we have 
\begin{equation}
  P_\Phi (y) = \frac{1}{Z} \prod_{i=1}^{N} \Phi_i(y_i) \prod_{(i,j)\in E} \Phi_{ij}(y_i,y_j)
  \end{equation}

Modeling $y_i^k$ as a continuous variable(higher value indicates higher confidence that segment i is of class k), we can define the log of the node potential as 
$ log \Phi_i(k) = (y_i^k -  w_n^k.x_i )^2$  and the log of the edge potential as $ log \Phi_{ij}(l,k) = w_e^{l,k}x_{ij} ( y_i^l -  y_j^k)^2$

Substituting these potential functions, we can write the log likelihood as: 

\begin{eqnarray*}
P_w (y|x) &=& \frac{1}{Z}exp(-\sum_{i=1}^{N} \sum_{k=1}^{K} (y_i^k - w^{k}_{n}.x_{i})^2 - \sum_{(i,j)\in E} \sum_{l,k=1}^{K} w_{e}^{l,k}.x_{ij}(y_i^l - y_j^k)^2), where\\
Z&=&\int_{-\infty}^{+\infty }{exp \bigg( - \sum_{i=1}^{N} \sum_{k=1}^{K} (y_i^k - w^{k}_{n}.x_{i})^2 - \sum_{(i,j)\in E} \sum_{l,k=1}^{K} w_{e}^{l,k}.x_{ij}(y_i^l - y_j^k)^2 \bigg) } d^{NK}y\\
&=&\int_{-\infty}^{+\infty }{exp \bigg(-y^T B y+a^T y-\frac{a^Ta}{4}\bigg) } d^{NK}y\\
&=&exp(-\frac{a^Ta}{4})\int_{-\infty}^{+\infty }{exp \bigg(-y^T B y+a^T y\bigg) } d^{NK}y\\
&=&exp(-\frac{a^Ta}{4})\frac{(\sqrt{\pi})^{NK}}{\sqrt{det(B)}}exp(\frac{1}{4}a^T B^{-1} a)\\
&=&exp\bigg(\frac{1}{4}a^T (B^{-1}-I) a+\frac{NK}{2}log(\pi)-\frac{log(det(B))}{2}\bigg)
\end{eqnarray*}
The second last-step uses a formula from wikipedia. \\
Sanity check: On removing the edge potentials, B=I and hence Z becomes independent of a(a depends on $w_n$).
\subsubsection{Learning}
In learning, we will find the maximum likelihood estimates of $w_n$ and $w_e$.
\begin{eqnarray*}
-log P_w (y|x) &=& \sum_{i=1}^{N} \sum_{k=1}^{K} (y_i^k - w^{k}_{n}.x_{i})^2 + \sum_{(i,j)\in E} \sum_{l,k=1}^{K} w_{e}^{l,k}.x_{ij}(y_i^l - y_j^k)^2) +log(Z)\\
&=& y^T B y-a^T y+\frac{a^Ta}{4} +log(Z)\\
&=& y^T B y-a^T y +\frac{a^Ta}{4}+\frac{1}{4}a^T (B^{-1} -I)a-\frac{log(det(B))}{2}+\frac{NK}{2}log(\pi)\\
&=& y^T B y-a^T y +\frac{1}{4}a^T B^{-1}a-\frac{log(det(B))}{2}+\frac{NK}{2}log(\pi)
\end{eqnarray*}
Note that B is a function of $w_e$ and a is a function of $w_n$.

So, we get:
\begin{eqnarray}
w=\arg\min_{w}:y^T B y-a^T y +\frac{1}{4}a^T B^{-1} a-\frac{log(det(B))}{2}
\end{eqnarray}
This function is not convex in B and a. We could still consider using iterative methods.

\begin{itemize}
\item If we fix $w_e$ such that B becomes a constant positive semi-definite matrix , the the problem becomes convex in a($w_n$). Note that the inverse of a +ve semi-definite matrix (if exists) is always +ve semi-definite(proved in appendix). $w_e^{l,k}x_{ij} \ge 0 \forall i,j,l,k \Rightarrow$ B is positive semidefinite.
\item if we fix $w_n$, the problem does not seem to be convex in B ($w_e$). We could start by using simulated annealing for this step. We can initialize $w_e$ with values such that $i^{th}$ element of $w_e^{l,k}$ is proportional to the number of times objects l and k occur in configuration i. This makes sure that the value of $w_e^{l,k}x_{i,j}$ is greater when i and j are of classes l and k respectively and hence penalty is high if  $(y_i^l==1 \&\& y_j^k==0) || (y_i^l==0 \&\& y_j^k==1)$
\end{itemize}

\subsubsection{Inference}
Inference boils down to a QP in y.

\subsection{Model Formulation 2}
Given labels $\{1,.., K\}$, $y_i^k$ is an indicator variable which is 1 if segment i has label k. 

For standard pair-wise Markov Network, we have 

\begin{equation}
  P_\Phi (y) = \frac{1}{Z} \prod_{i=1}^{N} \Phi_i(y_i) \prod_{(i,j)\in E} \Phi_{ij}(y_i,y_j)
  \end{equation}
  
  We can define the log of node potential as a linear function of the node features:  $log \Phi_i(k) = w_n^k.x_i$ , 
where $x_i$ is the feature vector of node i. Intuitively, the training process(margin-maximization) will try to pick $w_n^1...w_n^K$ such that $w_n^k.x_i>w_n^l.x_i \forall l$ if the correct label is k.\\
  Similarly, the log of edge potential can be defined as $log \Phi_{ij}(l,k) = w_{e}^{l,k}.x_{ij}$. The intuition is that $w_{e}^{l,k}$ will be high if segments of label k and label l appear frequently in configurations having relative features(relations) $x_{ij}$ . $x_{ij}$ could be a vector of indicator variables each indicating whether a particular relation holds between segments i and j.
However, instead that hard-coding the thresholds for relations like on-top, nearby, coplanar , we believe it would be better to include more informative continuous features in $x_{i,j}$ like difference of coordinates of centroids of the 2 segments, difference between mean normals etc.\\

Substituting these potential functions, we can write the log likelihood as: 

\begin{equation}
 \log P_w (y|x) = \sum_{i=1}^{N} \sum_{k=1}^{K} (w^{k}_{n}.x_{i})y_{i}^{k} + \sum_{(i,j)\in E} \sum_{l,k=1}^{K} (w_{e}^{l,k}.x_{ij})y_i^l y_j^k   -   \log Z_w(x)
\end{equation}

Doing some matrix manipulations(as shown in \cite{taskar2004learning}), we get:

\begin{equation}
 \log P_w (y|x) = wXy -logZ_w(x)
\end{equation}
where $w=[w_n w_e], y_n=[y_1^1...y_1^K....y_N^1...y_N^K],y_{i,j}^{l,k}=y_i^l*y_j^k,y_e=[ .....y_{i,j}^{1,1},y_{i,j}^{1,2}...y_{i,j}^{K,K} ....], y=[y_n,y_e]$ and X is constructed accordingly.
$w$ is of size $K*|x_i|+K^2*|x_{ij}|$

\subsubsection{Learning}
The idea is to maximize the margin by which probability of correct label-sequence beats the probability of any incorrect label-sequence.

\begin{eqnarray*}
min: \frac{1}{2} ||w||^2 + C\epsilon\\
s.t.: wX(\hat{y}-y)\ge \Delta(\hat{y}-y) -\epsilon \forall y\neq \hat{y}
\end{eqnarray*}
where, $\Delta(\hat{y}-y)$ counts the number of elements in which $\hat{y}$ and y differ.
The number of constraints is $K^N-1$. We hope that the cutting plane methods\cite{joachims2009cutting} 
which start by removing all constraints and  iteratively keeps adding most violated constraints would work.
\subsubsection{Inference}
Inference can be done in 2 ways. However, none of them have any optimality guarantees.

\subsubsubsection{LP rounding}
Prediction is an integer programming problem(which is NP complete):


\begin{eqnarray*}
y^*=\arg \max _y wXy\\
st: y_{i,j}^{l,k}\le y_i^l\\
y_{i,j}^{l,k}\le y_j^k\\
\forall i \sum_{j=1}^{K} y_i^j = 1
\end{eqnarray*}
 
We have to make sure that the coefficients of $y_{i,j}^{l,k}$ are positive in  y wXy.
% Or used the formulation using marginalization.

We hope we can prove that the LP-relaxation of this problem has provable approximation bounds.

\subsubsubsection{Graph Cuts}
Consider the log probability formula in original problem (ignoring terms not depending on y):
\begin{equation}
Energy(y)= -\log P_w (y|x) = -\sum_{i=1}^{N} \sum_{k=1}^{K} (w^{k}_{n}.x_{i})y_{i}^{k} - \sum_{(i,j)\in E} \sum_{l,k=1}^{K} (w_{e}^{l,k}.x_{ij})y_i^l y_j^k
\end{equation}
It is easy to see that this function is regular(submodular) if $w_e^{l,k}x_{ij} \ge 0 \forall i,j,l,k$ . Enforcing this constraint does not seem to cause any loss in generality. For objects that usually co-occur in a configuration ,learning algorithm can set $w_e^{l,k}x_{ij}$ higher so as to minimize energy when they occur together during inference. So, this energy can be minimized {\bf exactly} by using graph-cuts(the regularity constraints make sure that the corresponding graph has no negative edges). The only problem is that the optimal solution might have multiple indicators true for the same segment or none of the indicators true for a segment. We could add terms of the form $\infty y_i^ky_i^l \forall i, l\neq k$ but that breaks regularity.
Hopefully, the number of segments for which this happens is small and hopefully those segments form many disconnected components. In this case, an exhaustive search can be done for labels of only these segments.
If exhaustive search cannot be done, one heuristic could  be using the solution of LP rounding  to break the tie.


     
   \section{Experiments}
   \subsection{Data}
    RGB-D image sequences are recorded using Kinect camera sensors and they are stitched together into one point cloud using RGBD-slam.  We segment the data into regions as explained in the previous section. The segments are then then labelled with object labels. The labels which have sufficient number of examples were chosen. These are : \{ wall, floor, table, chair, tableBase, chairBase \}. We obtained stitched point clouds of 22 office scenes and labelled a total of 190 segments. We plan to collect more data for 30 different office spaces, collected on two different days, for our final evaluation. The data will be collected on different days in order to account for variability of the objects in the scene.

   \subsection{Results}
   The table \ref{tab:result_set1} shows the performance of our models with only the node potentials. The first model with only node potentials is equivalent to linear regression on indicator values for each class and the second model is equivalent to multi-class linear SVM. The first row shows the train and test accuracies for linear regression using all features. It can be seen that learning on all features has the problem of over-fitting the training data. The second row shows the performance of learning using only 9 features which are eigen-values, normal orientation wrt vertical, hue, intensity averaged over all points in a segment and the length, width and hight of the bounding box. The last row corresponds to the baseline which predicts the most frequent label from the train-data as the prediction always and it gives 33.46\% test accuracy. 
   

\begin{table}[!h]
 \label{tab:result_set1}
          \begin{center}
            \begin{tabular}{|c|c|c|c|c|c|c|c|} \hline
              &  \multicolumn {2}{|c|}{ Office Scenes }  &   \multicolumn {2}{|c|}{ Home Scenes } &  \multicolumn {2}{|c|}{ Combined Scenes }\\ \hline
               Algorithm  & Precision &  Recall & Precision &  Recall  & Precision &  Recall  \\ \hline
               linear\_regression &  &  &  & &  &\\  \hline
               svm\_multiclass & 54.39\%  & 54.39\% &  & &  &\\  \hline
                svm\_mrf & 43.95\% & 29.62\% &  & &  &\\ \hline
                 &  &  &  & &  &\\ \hline
            \end{tabular}
         \caption{Average Precision and Recall }
        \end{center} 
\end{table}

      \begin{table}[!h]
        \label{tab:result_set1}
          \begin{center}
            \begin{tabular}{|c|c|c|} \hline
               Algorithm  & Train Accuracy & Test Accuracy \\ \hline
               linear-55f & 96 & 41.67\\  \hline
               linear-9f & 75.03 &  63.85\\ \hline
               baseline-max-class  &  & 33.46 \\ \hline
            \end{tabular}
         \caption{Performance with only node potentials }
        \end{center} 
      \end{table}
      
      \newpage
 
% \section{Future Work And Timeline}

%Table \ref{tab:Timeline} shows a timeline for near future. After that, we would like to fix many issues with out current model. The 
main problem is that we are using a single set of weights for the entire object.
%This might be true for coherent objects like wall, ground etc. But for most of the real-world objects, it might not work. For example, a handle of a chair and it's backrest look quite different and it is unreasonable to try to fit the same linear model to both. In our dataset, these parts indeed end up as different segments.
% A part-based model for assembling parts of objects and using context for object relationships at higher level looks more promising.

 \small

 \bibliographystyle{abbrv}
\bibliography{references}

\end{document}
