\documentclass[11pt]{article} % For LaTeX2e
\usepackage{nips10submit_09,times}
\usepackage{cite}

\usepackage{ upgreek }
\usepackage{graphicx}
\usepackage{url}
\usepackage{verbatim} 

\newenvironment{itemize*}%
  {\begin{itemize}%
    \setlength{\itemsep}{0pt}%
    \setlength{\parskip}{0pt}}%
  {\end{itemize}}


\title{CS 6758: Semantic Labeling of Indoor Office Spaces using RGB-D Images}

\author{
Hema Swetha Koppula \\
 \texttt{hsk49@cornell.edu}
}
%\author{
%Abhishek Anand\\
% \texttt{aa755@cornell.edu}
% }
\nipsfinalcopy
\begin{document}
\maketitle

\begin{abstract}
We aim to use the high quality RGB-D images obtained by kinect cameras to imrpove state of the art of semantic labelling of indoor office spaces.
To get started, we  collected kinect-videos of 11 office scenes, stiched them to form large point-cloud models and labelled them manually.
We trained and tested a very simple linear model which acieves 65 \% accuracy in labelling into 6 categories. In addition to this, we formulated 2 MRF models which use context to infer labels and worked out the learning and inference mechanisms. 
\end{abstract}

\section{Introduction}


  
  The availability of  RGB-D (Kinect-style) cameras \cite{kinect} which are capable of providing high quality synchronized 
  videos of both color and depth presents a great opportunity to combine color- and depth-based recognition. We aim 
  to use this rich new data for automated indoor scene interpretation using semantic context modeling. Given an RGB-D 
  image, we would like to label each point in the scene with the semantic label of the object to which the point belongs
  (eg. wall, floor, table, computer, etc). Many state-of-the-art object detectors make use of only features local to the region being 
  considered, but these approaches do not work for cluttered scenes. We plan to model the relations between various objects 
  in the scene and make use of this contextual information for scene interpretation. In addition to this, we are doing learning and
  inference on large stitched 3D models containling multiple views so that we have more context cues.

   \subsection{Goal}
     The goal of the project is scene understanding of indoor office spaces. Given an RGB-D image of a office scene, each point in the 
     image is classified to a semantic label. The various semantic labels to be detected are : wall, floor, ceiling, doors, tables, furniture 
     (chairs, sofas), office supplies (books, pens, markers), wearable\_on\_ground(shoes, etc.), wearable\_rest (jackets, caps,etc.), 
     containers (cups, mugs, water bottles), computer (monitor, keyboard, mouse, cpu, laptop), clutter (the rest). Once the semantic labeling 
     is performed in a given scene, this information can be used by the robot to perform various tasks like, finding objects and fetching them, 
     or maintaining an inventory of objects in the office building, etc.

\begin{comment}
   \subsection{Method}
     The image is first segmented into smaller point clouds based on segmentation using both image and point cloud features. We define a
     MRF structure, where each segment forms a node in the graph and each segment is connected to its k-nearest neighbors via relation links.
     The various relations we define between objects are: orthogonal, parallel, adjacent, coplanar, self/same-object, above, below, on-top-of, 
     in-front-of, beside, in-proximity, etc. The node potentials will depend on local visual and shape features, and the edge potentials will depend 
     on the contextual relations between the two nodes.

      
   \subsection{Evaluation}
     We evaluate our scene understanding method by measuring the accuracy of semantic labeling on the test set ie., percent of objects labeled 
     correctly. The test sets will comprise of different types of office spaces: 1. new office spaces which weren't used in the training of the model, 
     2. similar office spaces but new rooms, and 
     3. same office spaces but data collected on a different day. 
     
 \end{comment}
     
     
 \section{Related Work}
 
 
    It has been shown that contextual information significantly improve the performance of vision-based 
   object detectors \cite{Torralba:exploting_context, Hoiem:puttingobjects,Torralba:contextualmodels}. However 
   little work has focused on 3-d context. Hoiem et al. \cite{Hoiem:puttingobjects} and Leibe et al. \cite{Leibe07:dynamic} 
  infer the camera location and scene geometry from a single 2-d image or stereo video stream, respectively. 
  These works reconstruct rough geometry of street scenes (pedestrians and cars) and cannot, for example, 
  be used for estimating 3-d features of small objects.
  
    Some novel works use 3-d point clouds from depth sensors for detecting objects in both indoor and outdoor environments 
    \cite {xiong:indoor,Rusu:ObjectMaps , xiong:3DSceneAnalysis, Golovinskiy:shape-basedrecognition, Shapovalov2010}. 
   ~\cite{xiong:indoor} presents a CRF based method to discover and exploit contextual information, classifying planar patches 
   extracted from the point cloud data. They model the key structural components of building interiors (walls, floors, and ceilings) 
   and on distinguishing these components from other objects in the environment. The algorithm uses local features as well as 
   contextual relationships such as orthogonal, parallel, adjacent, and coplanar to label the patches.
  
   Gould et al.  \cite{gould:fusion} and Quigley et al. ~\cite{quigley:high-accuracy} demonstrate that augmenting state-of-the-art
   computer vision techniques with high-resolution 3D information can enhance object detection in cluttered real-world environments. 
   They merge the depth and visual data to obtain color/intensity, depth (location in 3-d space),  and surface normal information
   for each pixel. From this data, the method in \cite{gould:fusion} uses a sliding window approach to extract  2d image features, 
   contextual cues (such as height above the ground) and 3-d features (such as object size). They then train a binary logistic 
   classifier based on the features as object detectors. ~\cite{quigley:high-accuracy} perform object detection using a extension of 
   the sliding-window  approach to combine the visual and depth channels. They combine this with a door opening system and 
   perform an inventory-control experiment. 

   The recent availability of high quality synchronized videos of both color and depth obtained from RGB-D (Kinect-style) 
    depth cameras \cite{kinect}, shifted the focus to designing object detection techniques making use of both visual as well as 
    shape features ~\cite{lai:icra11a, lai:icra11b}. Henry et al. ~\cite{deter:rgbd} present a full 3D mapping system
    that utilizes a novel joint optimization algorithm combining visual features and shape-based alignment from RGB-D data.

\section{Our approach}
We begin 
The image is first segmented into smaller point clouds based on segmentation using both image and point cloud features. We define a
     MRF structure, where each segment forms a node in the graph and each segment is connected to its k-nearest neighbors via relation links.
     The various relations we define between objects are: orthogonal, parallel, adjacent, coplanar, self/same-object, above, below, on-top-of, 
     in-front-of, beside, in-proximity, etc. The node potentials will depend on local visual and shape features, and the edge potentials will depend 
     on the contextual relations between the two nodes.

     
   \section{Experiments}
   \subsection{Data}
     RGB-D point clouds of 30 different office spaces collected on two different days. The RGB-D image sequences will be stitched together
     into one point cloud in order to obtain full information and cleaner data. The data is collected on different days in order to account for 
     variability of the objects in the scene. The data is then labeled with object labels and object-object relations.


 
 \small
 \bibliographystyle{abbrv}
\bibliography{references}

\end{document}